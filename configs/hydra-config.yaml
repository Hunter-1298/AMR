# Global configurations
project_name: Testing_Transformer
run_name: Tokenized_AMR:${now:%Y-%m-%d_%H-%M-%S}
train_tokenizer: False
mode: online
debug: False
log_model: True
seed: 42

# In your hydra config
hyperparams:
  # Training
  epochs: 100
  learning_rate: .0003
  batch_size: 64
  dropout: 0.25
  hidden_dim: 64
  feature_dim: 32
  num_classes: 11
  # etc.

# Paths
output_dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
data_dir: data/

Tokenizer:
  _target_: models.vq_vae.vq_vae.RFVQVAE
  commitment_cost: 0.25
  learning_rate: 0.001
  quantizer_amp:
    _target_: models.vq_vae.quantizer.VectorQuantizerEMA
    embedding_dim: 64
    codebook_size: 512
    beta: 0.25
    decay: 0.99
    epsilon: .00001
  quantizer_phase:
    _target_: models.vq_vae.quantizer.VectorQuantizerEMA
    embedding_dim: 64
    codebook_size: 512
    beta: 0.25
    decay: 0.99
    epsilon: .00001
  encoder_amp:
    _target_: models.vq_vae.encoder.Encoder
    hidden_dim: 128
    feature_dim: 64 #output feature space
  encoder_phase:
    _target_: models.vq_vae.encoder.Encoder
    hidden_dim: 128
    feature_dim: 64 #output feature space
  decoder_amp:
    _target_: models.vq_vae.decoder.Decoder
    embedding_dim: ${Tokenizer.quantizer_amp.embedding_dim}
    feature_dim: ${Tokenizer.encoder_amp.feature_dim}
    output_channels: 1
    output_dim: 128 #output feature space
  decoder_phase:
    _target_: models.vq_vae.decoder.Decoder
    embedding_dim: ${Tokenizer.quantizer_phase.embedding_dim}
    feature_dim: ${Tokenizer.encoder_phase.feature_dim}
    output_channels: 1
    output_dim: 128 #output feature space

Transformer:
  _target_: models.experts.attention.transformer_module.BidirectionalTransformer
  epochs: ${hyperparams.epochs}
  embed_dim: ${Tokenizer.quantizer_amp.embedding_dim}
  num_heads: 4
  num_layers: 8
  dropout: 0.2
  num_classes: ${hyperparams.num_classes}
  learning_rate: ${hyperparams.learning_rate}

Model:
  _target_: models.router.Router
  input_dim: 256
  hidden_dim: ${hyperparams.hidden_dim}
  feature_dim: ${hyperparams.feature_dim}
  lr: ${hyperparams.learning_rate}
  num_classes: ${hyperparams.num_classes}
  load_balance_experts: False
  experts:
    # - _target_: models.experts.spatial.simple_cnn.Spatial_CNN
    #   # hidden_dim: ${hyperparams.hidden_dim} #output feature space
    #   hidden_dim: ${hyperparams.hidden_dim} #output feature space
    #   feature_dim: ${hyperparams.feature_dim} #output feature space
    #   # feature_dim: ${hyperparams.feature_dim} #output feature space

    # - _target_: models.experts.temporal.simple_lstm.Temporal_LSTM
    #   input_size: 2 #num_channels
    #   hidden_dim: ${hyperparams.hidden_dim} #output feature space
    #   feature_dim: ${hyperparams.feature_dim} #output feature space
    #   num_layers: 2

    # - _target_: models.experts.frequency.simple_cnn.Frequency_CNN
    #   hidden_dim: ${hyperparams.hidden_dim} #output feature space
    #   feature_dim: ${hyperparams.feature_dim} #output feature space

    # - _target_: models.experts.statistical.simple_cumulants.Cumulants
    #   hidden_dim: ${hyperparams.hidden_dim} #output feature space
    #   feature_dim: ${hyperparams.feature_dim} #output feature space

    # - _target_: models.experts.constellation.simple_cnn.Constellation_CNN
    #   hidden_dim: ${hyperparams.hidden_dim} #output feature space
    #   feature_dim: ${hyperparams.feature_dim} #output feature space

  classifier:
    _target_: models.classifier.Classifier
    input_dim: ${hyperparams.feature_dim} #output feature space
    hidden_dim: ${hyperparams.hidden_dim}
    num_classes: ${hyperparams.num_classes}
    dropout: ${hyperparams.dropout}

dataset:
  iq: false
  batch_size: ${hyperparams.batch_size}
  num_workers: 4
  train_val_split: [0.7, 0.3]
  random_seed: ${seed}
