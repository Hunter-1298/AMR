# Global configurations
project_name: Contrastvie_Bottleneck
run_name: Different_Timestep_timestep_predictor_Classifier_Free
train_encoder: False
# Latent Diffusion Settings #
contrastive_encoder: True
train_diffusion: False
conditon: True
diffusion_contrastive: "bottleneck" #"output"
vis_diffusion: False
# Classifier Settings #
train_classifier: True
classifier_free: True
fine_tune_diffusion: False
# Checkpoints to load #
encoder_checkpoint_name: moco_encoder_val_loss=7.82.ckpt
# checkpoint for reconstruction vae
# encoder_checkpoint_name: vae_phaseepoch=96_val_loss=0.0005596.ckpt
# diffusion_checkpoint_name: contrastive_diffusion_no_condition_val_loss=0.2796.ckpt
# diffusion_checkpoint_name: contrastive_diffusion_bottleneck_contrastive_condition_val_loss=0.3436.ckpt
# diffusion_checkpoint_name: denoised_vs_clean.ckpt
# diffusion_checkpoint_name: contrastive_diffusion_condition_val_loss=0.2765.ckpt
# diffusion_checkpoint_name: diffusion_condition_3MParams_epoch=181_val_loss=0.0757.ckpt
# diffusion_checkpoint_name: diffusion_no_condition_2.3MParams_epoch=191_val_loss=0.0702.ckpt
diffusion_checkpoint_name: contrastive_diffusion_denoised_contrastive_condition_val_loss=0.3279.ckpt
# diffusion_checkpoint_name: contrastive_diffusion_denoised_contrastive_noncondition_val_loss=0.3309.ckpt
mode: online
debug: False
log_model: True
seed: 42

# In your hydra config
hyperparams:
    # Training
    epochs: 200
    classifier_epochs: 100
    learning_rate: .001
    batch_size: 1024
    contrastive_batch_size: 4096 # make this high so we can use all negatives in batch without increase batch for diffusion model
    dropout: 0.25
    hidden_dim: 32
    feature_dim: 16
    num_classes: 11

# Paths
output_dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
data_dir: data/

# Encoder_Reconstruction:
#     _target_: models.latent_encoder_models.latent_reconstruction_encoder.LatentEncoderReconstrction
#     learning_rate: .0005
#     encoder:
#         _target_: models.latent_encoder_models.resnet_encoder.ResNet1D
#     decoder:
#         _target_: models.latent_encoder_models.decoder.Decoder1D

Encoder:
    _target_: models.latent_encoder_models.MoCoV3Encoder.MoCoV3Encoder
    learning_rate: .0001
    encoder:
        _target_: models.latent_encoder_models.resnet_encoder.ResNet1D

Diffusion:
    _target_: models.diffusion.latent_diffusion.LatentDiffusion
    learning_rate: ${hyperparams.learning_rate}
    n_steps: 500
    linear_start: 0.0001 # starting noise value for beta schedule
    linear_end: 0.01 # ending value for the beta schedule
    weight_decay: 1e-2
    latent_scaling: 1.1981 # contrastive scaling # 2.98697 - reconstruction encoder # False - set to false if we want to recalcualte
    diffusion_contrastive: ${diffusion_contrastive}
    unet:
        _target_: models.diffusion.unet_1d.UNet1DModel
        sample_size: 64
        in_channels: 32
        out_channels: 32
        down_block_types:
            ["DownResnetBlock1D", "AttnDownBlock1D", "AttnDownBlock1D"]
        up_block_types: ["AttnUpBlock1D", "AttnUpBlock1D", "UpResnetBlock1D"]
        mid_block_type: "UNetMidBlock1D"
        block_out_channels: [32, 64, 128]
        layers_per_block: 3
        condition: ${conditon}
        conditional: 11 # Class conditioning

Classifier:
    _target_: models.classifier.classifier.LatentClassifier
    learning_rate: 0.001
    beta: 0.25 # temperature for sigmoid of snr -> timestep
    fine_tune_diffusion: ${fine_tune_diffusion}
    num_classes: ${hyperparams.num_classes}
    classifier_free: ${classifier_free}
    classifier_head:
        _target_: models.classifier.conv_classifier.Conv1DHead
        num_classes: ${hyperparams.num_classes}

dataset:
    iq: true #true
    batch_size: ${hyperparams.batch_size}
    contrastive_batch_size: ${hyperparams.contrastive_batch_size}
    num_workers: 12
    train_val_split: [0.8, 0.2]
    random_seed: ${seed}
