project_name: VQ-VAE
run_name: 32_Feature_Dim:${now:%Y-%m-%d_%H-%M-%S}
train_tokenizer: true
mode: online
debug: true
log_model: false
seed: 42
hyperparams:
  epochs: 15
  learning_rate: 0.009
  batch_size: 32
  dropout: 0.25
  hidden_dim: 64
  feature_dim: 32
  num_classes: 11
output_dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
data_dir: data/
Tokenizer:
  _target_: models.vq_vae.vq_vae.RFVQVAE
  commitment_cost: 0.25
  learning_rate: 0.01
  quantizer_amp:
    _target_: models.vq_vae.quantizer.VectorQuantizer
    embedding_dim: 64
    codebook_size: 512
    beta: 0.25
  quantizer_phase:
    _target_: models.vq_vae.quantizer.VectorQuantizer
    embedding_dim: 64
    codebook_size: 512
    beta: 0.25
  encoder_amp:
    _target_: models.vq_vae.encoder.Encoder
    hidden_dim: 128
    feature_dim: 32
  encoder_phase:
    _target_: models.vq_vae.encoder.Encoder
    hidden_dim: 128
    feature_dim: 32
  decoder_amp:
    _target_: models.vq_vae.decoder.Decoder
    embedding_dim: ${Tokenizer.quantizer_amp.embedding_dim}
    latent_dim: ${Tokenizer.encoder_amp.feature_dim}
    output_channels: 1
    output_dim: 128
  decoder_phase:
    _target_: models.vq_vae.decoder.Decoder
    embedding_dim: ${Tokenizer.quantizer_phase.embedding_dim}
    latent_dim: ${Tokenizer.encoder_phase.feature_dim}
    output_channels: 1
    output_dim: 128
dataset:
  iq: false
  batch_size: ${hyperparams.batch_size}
  num_workers: 4
  train_val_split:
  - 0.7
  - 0.3
  random_seed: ${seed}
